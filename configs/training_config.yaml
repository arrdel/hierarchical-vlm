# HierarchicalVLM Training Configuration
# Comprehensive configuration for training with all three phases integrated:
# - Phase 4: Efficient Attention Mechanisms
# - Phase 5: Domain-Specific Fine-Tuning (LoRA + Multi-Task Heads)
# - Phase 1: Adaptive Token Merging (Motion + Saliency)

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  # Base model configuration
  hidden_size: 768
  num_attention_heads: 12
  intermediate_size: 3072
  num_hidden_layers: 12
  
  # Efficient attention configuration
  attention:
    type: "hierarchical"  # Options: standard, strided, local_global, cross_memory, performer, mamba, hierarchical
    
    sparse_attention:
      stride: 4
      window_size: 64
      
    linear_attention:
      kernel_type: "elu"  # FAVOR+ kernel
      
    hierarchical:
      num_levels: 3
      level_ratios: [1.0, 0.5, 0.25]
  
  # Domain modules configuration
  domain_modules:
    enabled: true
    num_domains: 4  # sports, tutorials, news, general
    
    lora:
      rank: 8
      alpha: 16
      dropout: 0.05
      target_layers: ["attention", "mlp"]
    
    heads:
      action_detection:
        enabled: true
        num_classes: 150
        num_frames: 32
        
      video_qa:
        enabled: true
        num_answers: 1000
        max_questions: 10
        
      video_captioning:
        enabled: true
        vocab_size: 10000
        max_length: 100
      
      multi_task:
        enabled: true
        task_weights: [0.33, 0.33, 0.34]
    
    expert_routing:
      routing_type: "soft"  # Options: hard, soft
      specialization_levels: [0.9, 0.8, 0.7, 0.5]  # For each domain
  
  # Token merging configuration
  token_merging:
    enabled: true
    
    motion:
      enabled: true
      window_size: 15
      normalize: true
      
    saliency:
      enabled: true
      edge_saliency: true
      attention_saliency: true
      color_saliency: true
      fusion_method: "weighted_sum"
    
    adaptive_merging:
      target_compression_ratio: 0.5
      min_compression_ratio: 0.25
      max_compression_ratio: 1.0
      motion_scale: 0.5
      saliency_scale: 0.5

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  # Data loading
  batch_size: 32
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
  
  # Video preprocessing
  video:
    num_frames: 32
    frame_size: 224
    frame_sampling: "uniform"  # Options: uniform, random, temporal
    
  # Text preprocessing
  text:
    max_length: 77
    tokenizer: "clip"
  
  # Data augmentation
  augmentation:
    enabled: true
    random_crop: true
    color_jitter: 0.4
    random_flip: true
    temporal_shift: true
    dropout: 0.1

# ============================================================================
# OPTIMIZER CONFIGURATION
# ============================================================================
optimizer:
  type: "adamw"  # Options: adam, adamw, sgd
  lr: 1.0e-4
  weight_decay: 1.0e-5
  betas: [0.9, 0.999]
  eps: 1.0e-8
  
  # Layer-wise learning rate adjustment
  layer_decay: 0.85
  
  # Gradient accumulation
  accumulation_steps: 1
  gradient_clip: 1.0

# ============================================================================
# SCHEDULER CONFIGURATION
# ============================================================================
scheduler:
  type: "cosine"  # Options: cosine, linear, step, exponential
  T_max: 100
  eta_min: 1.0e-6
  
  # Warmup
  warmup_steps: 1000
  warmup_type: "linear"  # Options: linear, constant

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  num_epochs: 100
  log_interval: 100
  eval_interval: 1  # Evaluate every N epochs
  save_interval: 1
  
  # Mixed precision training
  mixed_precision: true
  amp_dtype: "float16"  # Options: float16, bfloat16
  
  # Gradient checkpointing
  gradient_checkpointing: true
  
  # Loss functions
  loss:
    action_detection: "cross_entropy"
    video_qa: "cross_entropy"
    video_captioning: "nll"
    
  # Domain balancing
  domain_balancing:
    enabled: true
    sample_strategy: "weighted"  # Options: weighted, balanced
    domain_weights: [0.25, 0.25, 0.25, 0.25]
  
  # Temporal smoothing
  temporal_smoothing:
    enabled: true
    window_size: 3

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  # Metrics
  metrics:
    action_detection: ["mAP", "recall@0.5", "recall@0.75"]
    video_qa: ["accuracy", "f1"]
    video_captioning: ["bleu", "meteor", "cider"]
  
  # Evaluation settings
  eval_batch_size: 64
  eval_num_workers: 4
  
  # Best model tracking
  best_metric: "val_loss"
  patience: 10  # Early stopping

# ============================================================================
# CHECKPOINTING CONFIGURATION
# ============================================================================
checkpoint:
  save_dir: "./checkpoints"
  save_best: true
  save_last: true
  keep_top_k: 3
  
  # Resume configuration
  resume: null  # Set to checkpoint path to resume training
  
  # EMA (Exponential Moving Average)
  use_ema: true
  ema_decay: 0.999

# ============================================================================
# DISTRIBUTED TRAINING CONFIGURATION
# ============================================================================
distributed:
  enabled: false
  backend: "nccl"  # Options: nccl, gloo
  find_unused_parameters: true
  
  # DDP configuration
  ddp:
    sync_bn: true
    broadcast_buffers: false

# ============================================================================
# DEVICE CONFIGURATION
# ============================================================================
device:
  type: "cuda"  # Options: cuda, cpu
  gpu_id: 0
  seed: 42
  deterministic: true
  benchmark: false

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  level: "INFO"
  log_dir: "./logs"
  wandb:
    enabled: false
    project: "hierarchicalvlm"
    entity: null
  tensorboard:
    enabled: true
    log_dir: "./runs"
