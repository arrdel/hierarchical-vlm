# Domain Modules Configuration
# This file specifies parameters for LoRA, task heads, and domain experts

# ============================================================================
# LoRA Adapter Configuration
# ============================================================================
lora:
  # Low-rank adaptation rank
  rank: 8
  
  # Scaling factor (alpha / rank determines LoRA contribution magnitude)
  # Default: 2 Ã— rank (e.g., 16 for rank=8)
  alpha: 16.0
  
  # Dropout probability for regularization
  dropout: 0.05
  
  # Module names to apply LoRA to (None = all compatible modules)
  target_modules: null
  
  # If true, freeze base model and only train LoRA
  lora_only: true
  
  # Expansion factor for intermediate layers
  expansion_factor: 4.0

# ============================================================================
# Task-Specific Head Configuration
# ============================================================================
action_detection:
  # Number of action classes (ActivityNet: 200, THUMOS: 22)
  num_classes: 150
  
  # Number of video frames
  num_frames: 32
  
  # Use temporal convolution for context
  use_temporal_conv: true
  
  # Dropout probability
  dropout: 0.1

video_qa:
  # Number of possible answers
  num_answers: 1000
  
  # Use attention for frame weighting
  use_attention: true
  
  # Dropout probability
  dropout: 0.1
  
  # Number of attention heads
  num_heads: 8

video_captioning:
  # Vocabulary size for caption generation
  vocab_size: 10000
  
  # Maximum caption length
  max_caption_len: 20
  
  # Dimension for caption embeddings
  caption_dim: 512
  
  # Use LSTM decoder
  use_decoder: true
  
  # Dropout probability
  dropout: 0.1

# ============================================================================
# Domain Expert Configuration
# ============================================================================
domain_experts:
  # List of domains to specialize for
  domains:
    - name: sports
      # How much to specialize (0=general, 1=fully specialized)
      specialization_factor: 0.9
      expert_dim: 1024
      
    - name: tutorials
      specialization_factor: 0.8
      expert_dim: 1024
      
    - name: news
      specialization_factor: 0.7
      expert_dim: 1024
      
    - name: general
      specialization_factor: 0.5
      expert_dim: 1024
  
  # Routing strategy: 'hard' (one-hot) or 'soft' (weighted)
  routing_strategy: soft
  
  # Dropout probability
  dropout: 0.1

# ============================================================================
# Domain Router Configuration
# ============================================================================
domain_router:
  # Hidden dimension for routing network
  hidden_dim: 512
  
  # Return confidence scores from router
  return_confidence: true
  
  # Dropout probability
  dropout: 0.1

# ============================================================================
# Training Configuration
# ============================================================================
training:
  # Learning rate for LoRA fine-tuning
  lora_lr: 5.0e-4
  
  # Learning rate for task heads
  head_lr: 1.0e-3
  
  # Optimizer: 'adam' or 'adamw'
  optimizer: adamw
  
  # Weight decay for regularization
  weight_decay: 0.01
  
  # Gradient clipping max norm
  max_grad_norm: 1.0
  
  # Number of training epochs
  num_epochs: 10
  
  # Batch size
  batch_size: 32
  
  # Number of warmup steps
  warmup_steps: 100
  
  # Learning rate schedule: 'linear', 'cosine', 'constant'
  lr_schedule: cosine

# ============================================================================
# Multi-Task Loss Weights
# ============================================================================
loss_weights:
  action_detection: 1.0
  video_qa: 0.5
  video_captioning: 0.3

# ============================================================================
# Model Integration
# ============================================================================
integration:
  # Dimensions from LongVLM backbone
  backbone_dim: 768
  
  # Number of attention heads in backbone
  num_heads: 12
  
  # Hidden dimension in FFN
  hidden_dim: 3072
  
  # Dropout probability
  dropout: 0.1
  
  # Layer normalization epsilon
  layer_norm_eps: 1.0e-6

# ============================================================================
# Data Configuration
# ============================================================================
data:
  # Video frame rate (fps)
  fps: 2
  
  # Target number of frames per video
  num_frames: 32
  
  # Input frame size (height, width)
  frame_size: [224, 224]
  
  # Number of workers for data loading
  num_workers: 4
  
  # Pin memory for faster GPU loading
  pin_memory: true

# ============================================================================
# Inference Configuration
# ============================================================================
inference:
  # Merge LoRA weights into base model after training
  merge_lora: true
  
  # Use FP16 for inference
  fp16: false
  
  # Batch size for inference
  batch_size: 64
  
  # Number of workers for inference
  num_workers: 4

# ============================================================================
# Logging and Checkpointing
# ============================================================================
logging:
  # Log directory
  log_dir: ./logs
  
  # Logging frequency (steps)
  log_frequency: 10
  
  # Checkpoint directory
  checkpoint_dir: ./checkpoints
  
  # Save checkpoint frequency (epochs)
  save_frequency: 1
  
  # Keep only top-k checkpoints
  keep_top_k: 3
  
  # Enable tensorboard logging
  use_tensorboard: true
  
  # Tensorboard log directory
  tensorboard_dir: ./runs

# ============================================================================
# Evaluation Configuration
# ============================================================================
evaluation:
  # Evaluation frequency (steps)
  eval_frequency: 500
  
  # Metrics to compute
  metrics:
    - accuracy
    - f1_score
    - precision
    - recall
  
  # Threshold for action detection confidence
  action_confidence_threshold: 0.5
  
  # Threshold for IoU in temporal localization
  iou_threshold: 0.5

# ============================================================================
# Hardware Configuration
# ============================================================================
hardware:
  # Device: 'cuda' or 'cpu'
  device: cuda
  
  # Number of GPUs
  num_gpus: 1
  
  # Gradient accumulation steps
  gradient_accumulation_steps: 1
  
  # Mixed precision training (amp)
  amp: false
  
  # Distributed data parallel
  distributed: false

# ============================================================================
# Experimental Settings
# ============================================================================
experimental:
  # Random seed for reproducibility
  seed: 42
  
  # Enable debug mode (verbose logging)
  debug: false
  
  # Profiling mode
  profiling: false
  
  # Benchmark mode (faster, less stable)
  benchmark: true
