# Sparse Attention Configuration

# Strided Attention
strided:
  stride: 4
  dim: 768
  num_heads: 12

# Local + Global Attention
local_global:
  local_window: 64
  num_global_tokens: 4
  dim: 768
  num_heads: 12

# Cross Memory Attention
cross_memory:
  dim: 768
  num_heads: 12
  fusion_method: "attention"  # attention or concatenation
