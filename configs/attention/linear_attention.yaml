# Linear Attention Configuration

# Performer (FAVOR+)
performer:
  dim: 768
  num_heads: 12
  num_random_features: 256
  kernel_type: "elu"  # elu or relu

# Mamba
mamba:
  dim: 768
  state_size: 16
  expand_factor: 2
  dt_init: "constant"
  dt_init_floor: 1e-4
