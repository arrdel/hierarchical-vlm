# HierarchicalVLM - Project Structure

## âœ… Clean Project Organization

### Root Files (Active)
- `setup.py` - Package setup for installation
- `SETUP_DATASET_SCRATCH.sh` - Dataset extraction and organization script
- `RUN_TESTS.sh` - Test runner script
- `README.md` - Project documentation
- `requirements.txt` - Python dependencies

### Directories

```
â”œâ”€â”€ .trash/                       # Archived/unused scripts
â”‚   â”œâ”€â”€ SETUP_DATASET.sh
â”‚   â”œâ”€â”€ MONITOR_DOWNLOAD.sh
â”‚   â”œâ”€â”€ START_TRAINING.sh
â”‚   â”œâ”€â”€ train_example.py
â”‚   â””â”€â”€ training.log
â”‚
â”œâ”€â”€ hierarchicalvlm/              # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ video_conversation.py
â”‚   â”œâ”€â”€ data/                     # Data loading
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ activitynet_features_loader.py
â”‚   â”œâ”€â”€ model/                    # Model architecture
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ longvlm.py
â”‚   â”‚   â”œâ”€â”€ consolidate.py
â”‚   â”‚   â”œâ”€â”€ make_delta.py
â”‚   â”‚   â”œâ”€â”€ merge.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ train/                    # Training scripts
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train_features.py     # â­ Main training script (2-GPU DDP + W&B)
â”‚   â”‚   â”œâ”€â”€ llava_trainer.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ train_mem.py
â”‚   â”‚   â””â”€â”€ llama_flash_attn_monkey_patch.py
â”‚   â”œâ”€â”€ eval/                     # Evaluation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_utils.py
â”‚   â”‚   â”œâ”€â”€ run_inference_benchmark.py
â”‚   â”‚   â””â”€â”€ run_inference_qa.py
â”‚   â””â”€â”€ quantitative_evaluation/
â”‚
â”œâ”€â”€ configs/                      # Training configurations
â”‚   â””â”€â”€ training_config.yaml
â”‚
â”œâ”€â”€ data/                         # Dataset symlink
â”‚   â””â”€â”€ raw â†’ /media/scratch/adele/activitynet/
â”‚
â”œâ”€â”€ runs/                         # Training outputs
â”‚   â””â”€â”€ feature_training_v1_2gpu/  # Current training run
â”‚
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_attention.py
â”‚   â”œâ”€â”€ test_domain_modules.py
â”‚   â”œâ”€â”€ test_hierarchical_model.py
â”‚   â””â”€â”€ test_token_merging.py
â”‚
â”œâ”€â”€ datasets/                     # Dataset lists
â”‚   â””â”€â”€ anet/
â”‚
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ apply_delta.py
â”‚   â”œâ”€â”€ convert_instruction_json_to_training_format.py
â”‚   â”œâ”€â”€ filtering_pkl.py
â”‚   â””â”€â”€ save_features.py
â”‚
â””â”€â”€ docs/                         # Documentation
```

## ğŸ“Š Dataset Structure

```
/media/scratch/adele/activitynet/ActivityNet-13/
â”œâ”€â”€ gt.json                       (9.3 MB - annotations)
â”œâ”€â”€ train/
â”‚   â””â”€â”€ train/                    (9,032 .npy feature files)
â””â”€â”€ test/
    â””â”€â”€ test/                     (4,427 .npy feature files)
```

## ğŸš€ Quick Start - Training

### Single GPU
```bash
cd /home/adelechinda/home/projects/HierarchicalVLM
conda activate hierarchical_vlm
python hierarchicalvlm/train/train_features.py \
    --batch-size 32 \
    --num-epochs 50
```

### Multi-GPU (2 GPUs) with W&B Logging
```bash
conda run -n hierarchical_vlm torchrun --nproc_per_node=2 \
    hierarchicalvlm/train/train_features.py \
    --batch-size 32 \
    --num-epochs 50 \
    --wandb-project "hierarchical-vlm-features" \
    --wandb-run-name "activitynet_2gpu_training"
```

## ğŸ§¹ Cleanup Summary

**Archived to `.trash/`:**
- âœ… SETUP_DATASET.sh (original dataset setup)
- âœ… MONITOR_DOWNLOAD.sh (download monitoring)
- âœ… START_TRAINING.sh (old training launcher)
- âœ… train_example.py (example training code)
- âœ… training.log (old logs)

**Kept Active:**
- âœ… `SETUP_DATASET_SCRATCH.sh` - For dataset organization
- âœ… `RUN_TESTS.sh` - For testing
- âœ… `hierarchicalvlm/train/train_features.py` - Main training script

## ğŸ“‹ Training Features

âœ… Multi-GPU support (2 GPUs)
âœ… Distributed Data Parallel (DDP)
âœ… Weights & Biases integration
âœ… Verbose logging at batch and epoch level
âœ… Gradient norm tracking
âœ… Learning rate scheduling
âœ… Checkpoint saving (best + periodic)
âœ… Feature-based training (pre-extracted)
âœ… Attention masking support

## ğŸ¯ Ready to Train!

All setup complete. Project is organized and ready for training.

Generated: December 13, 2025
