<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="HierarchicalVLM: Efficient Long-Context Video Understanding via Hierarchical Temporal Aggregation.">
  <meta property="og:title" content="HierarchicalVLM - Efficient Long-Context Video Understanding" />
  <meta property="og:description" content="28.4% improvement in temporal consistency, 34.2% in downstream tasks." />
  <meta property="og:url" content="https://github.com/arrdel/hierarchical-vlm" />
  <meta name="twitter:title" content="HierarchicalVLM - Efficient Long-Context Video Understanding">
  <meta name="twitter:description" content="28.4% temporal consistency improvement, 34.2% downstream task improvement.">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords"
    content="video understanding, temporal learning, contrastive learning, hierarchical aggregation, transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HierarchicalVLM: Efficient Long-Context Video Understanding</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    html {
      scroll-behavior: smooth;
    }

    body {
      font-family: 'Noto Sans', sans-serif;
    }

    code {
      background-color: #f5f5f5;
      padding: 2px 6px;
      border-radius: 3px;
      font-size: 0.9em;
      color: #d63384;
    }

    pre {
      background-color: #f5f5f5;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
    }

    pre code {
      background-color: transparent;
      color: #333;
      padding: 0;
    }

    .table {
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
      margin: 1.5rem 0;
    }

    .table th {
      font-weight: 600;
      background-color: #f9f9f9;
    }

    .table tr:hover {
      background-color: #fafafa;
    }

    section.section {
      padding: 3rem 1.5rem;
    }

    .content a {
      color: #3273dc;
      text-decoration: none;
      border-bottom: 1px solid transparent;
      transition: border-bottom 0.2s ease;
    }

    .content a:hover {
      border-bottom: 1px solid #3273dc;
    }

    .content ul,
    .content ol {
      line-height: 1.8;
      margin-left: 1.5rem;
    }

    .content li {
      margin-bottom: 0.5rem;
    }

    .title.is-3,
    .title.is-4 {
      margin-top: 2rem;
      margin-bottom: 1rem;
      font-weight: 700;
    }

    .placeholder-box {
      border: 2px dashed #ccc;
      padding: 2rem;
      text-align: center;
      background-color: #f9f9f9;
      border-radius: 8px;
      margin: 2rem 0;
    }

    .placeholder-box .placeholder-icon {
      font-size: 4rem;
      color: #ccc;
      margin-bottom: 1rem;
    }

    .placeholder-box .placeholder-text {
      color: #666;
      font-size: 1.1rem;
      font-style: italic;
    }

    .publication-title {
      background: black;
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      font-weight: 700;
    }

    .hero.teaser {
      background: linear-gradient(to bottom, #f8f9fa, #ffffff);
    }

    .hero.is-light {
      background: linear-gradient(to bottom, #f0f4f8, #ffffff);
    }

    .link-block {
      display: inline-block;
      margin: 0.5rem 0.5rem 0.5rem 0;
    }

    .link-block .button {
      transition: all 0.3s ease;
    }

    .link-block .button:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    }

    .table {
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
      margin: 1.5rem 0;
    }

    .pdf-container {
      background: #f8f9fa;
      padding: 2rem;
      border-radius: 8px;
      margin-top: 2rem;
    }

    .section-divider {
      height: 4px;
      background: black;
      border: none;
      margin: 3rem auto;
      max-width: 200px;
      border-radius: 2px;
    }

    .metric-box {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      padding: 1.5rem;
      border-radius: 8px;
      margin-bottom: 1rem;
      box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
      text-align: center;
      transition: transform 0.3s ease;
    }

    .metric-box:hover {
      transform: translateY(-4px);
      box-shadow: 0 8px 16px rgba(102, 126, 234, 0.4);
    }

    .metric-value {
      font-size: 2.5rem;
      font-weight: bold;
      margin-bottom: 0.5rem;
    }

    .metric-label {
      font-size: 0.95rem;
      opacity: 0.95;
    }

    .equation-block {
      background: #f5f5f5;
      padding: 1.5rem;
      border-left: 4px solid #000;
      border-radius: 4px;
      margin: 1.5rem 0;
      overflow-x: auto;
      display: flex;
      justify-content: center;
      align-items: center;
    }

    .equation-block mjx-container {
      overflow-x: auto;
    }

    .equation-block .mathjax-display {
      margin: 0.5rem 0;
    }

    .challenge-item {
      background: #f9f9f9;
      padding: 1rem;
      border-radius: 6px;
      margin-bottom: 1rem;
      border-left: 4px solid #000;
    }

    .method-phase {
      background: linear-gradient(135deg, rgba(102, 126, 234, 0.05) 0%, rgba(118, 75, 162, 0.05) 100%);
      padding: 1.5rem;
      border-radius: 8px;
      margin: 1.5rem 0;
    }

    .result-card {
      background: white;
      padding: 1.5rem;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
      margin-bottom: 1.5rem;
    }

    .footer {
      background-color: #f5f5f5;
      padding: 2rem 1.5rem;
      border-top: 1px solid #e5e5e5;
    }

    .footer-content {
      max-width: 1240px;
      margin: 0 auto;
    }

    .highlight {
      background: #fffacd;
      padding: 0.2rem 0.4rem;
      border-radius: 2px;
    }

    @media (max-width: 768px) {
      .metric-box {
        margin-bottom: 1rem;
      }

      .title.is-1 {
        font-size: 2rem;
      }

      section.section {
        padding: 2rem 1rem;
      }
    }

    /* Additional enhancements */
    .stat-box {
      display: inline-block;
      background: linear-gradient(135deg, rgba(0, 0, 0, 0.02) 0%, rgba(0, 0, 0, 0.05) 100%);
      padding: 1rem 1.5rem;
      border-radius: 6px;
      margin: 0.5rem;
      border: 1px solid #e5e5e5;
      text-align: center;
    }

    .stat-number {
      font-size: 1.8rem;
      font-weight: bold;
      color: #000;
      display: block;
    }

    .stat-label {
      font-size: 0.85rem;
      color: #666;
      margin-top: 0.5rem;
    }

    .highlight-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 1rem;
      margin: 2rem 0;
    }

    .feature-box {
      background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
      padding: 1.5rem;
      border-radius: 8px;
      border: 1px solid #e5e5e5;
      transition: all 0.3s ease;
    }

    .feature-box:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      border-color: #000;
    }

    .feature-icon {
      font-size: 2rem;
      margin-bottom: 0.5rem;
    }

    .feature-title {
      font-weight: 600;
      font-size: 1rem;
      margin-bottom: 0.5rem;
      color: #000;
    }

    .feature-desc {
      font-size: 0.9rem;
      color: #666;
      line-height: 1.5;
    }

    .badge {
      display: inline-block;
      background: #000;
      color: white;
      padding: 0.3rem 0.8rem;
      border-radius: 20px;
      font-size: 0.8rem;
      font-weight: 600;
      margin-right: 0.5rem;
      margin-bottom: 0.5rem;
    }

    .stats-container {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 1rem;
      margin: 2rem 0;
    }

    .pub-links-enhanced {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 1rem;
      margin: 2rem 0;
    }
  </style>
</head>

<body>



  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HierarchicalVLM: Efficient Long-Context Video Understanding via
              Hierarchical Temporal Aggregation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- Paper authors -->
              <!-- <span class="author-block">
                <a href="https://github.com/arrdel" target="_blank">Adele Chinda</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com" target="_blank">Richmond Azumah</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com" target="_blank">Hemanth Demakethepalli Venkateswara</a></span> -->
              <p>Anonymous submission</p>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Georgia State University<br></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">


                <!-- Paper PDF link -->
                <span class="link-block">
                  <a href="https://arrdel.github.io/hierarchical_vlm/static/pdfs/submission.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/arrdel/hierarchical-vlm" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- WandB link -->
                <span class="link-block">
                  <a href="https://wandb.ai/el_chindah/hierarchical-vlm?nw=nwuserel_chindah1" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-chart-line"></i>
                    </span>
                    <span>W&B Logs</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/teaser.png" style="width:100%; border-radius: 8px" />
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          HierarchicalVLM efficiently models long-context video sequences via hierarchical temporal
          aggregation combined with temporal contrastive learning and vision-language alignment.
        </h2>
      </div>
    </div>

    <!-- Abstract Section -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>Understanding long-context video sequences remains a fundamental challenge in computer vision,
                requiring models to capture temporal dependencies across hundreds of frames while maintaining
                computational efficiency. We propose <strong>HierarchicalVLM</strong>, a novel architecture
                combining temporal contrastive learning with hierarchical feature aggregation and vision-language
                alignment for long-form video understanding.</p>

              <p>Our key innovation is a <strong>multi-scale temporal pooling mechanism</strong> that preserves
                temporal structure while progressively reducing computational complexity. We introduce a temporal
                contrastive loss that encourages frame-level consistency, coupled with a collapse-prevention
                regularizer to maintain representation diversity. The approach is evaluated on
                <strong>ActivityNet-1.3</strong>, a large-scale dataset with 13,459 videos spanning diverse
                activities.
              </p>

              <p>Experimental results demonstrate substantial improvements over baseline approaches: our method
                achieves <strong>28.4% improvement in temporal consistency metrics</strong> and <strong>34.2%
                  improvement in downstream activity classification</strong>. We further conduct comprehensive
                ablation studies validating each component contribution. Our implementation scales efficiently on
                multi-GPU systems, achieving <strong>1,575 samples/second throughput</strong> with linear scaling in
                batch size.</p>
            </div>
          </div>
    </section>

    <!-- Key Highlights Section -->
    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Key Highlights</h2>
            <hr class="section-divider">
            <div class="highlight-grid">
              <div class="feature-box">
                <div class="feature-icon">ðŸ“Š</div>
                <div class="feature-title">28.4% Improvement</div>
                <div class="feature-desc">Temporal consistency over baseline transformers</div>
              </div>
              <div class="feature-box">
                <div class="feature-icon">âš¡</div>
                <div class="feature-title">130x Memory Reduction</div>
                <div class="feature-desc">Hierarchical pooling reduces complexity from O(TÂ²) to O(T)</div>
              </div>
              <div class="feature-box">
                <div class="feature-icon">ðŸš€</div>
                <div class="feature-title">1,575 Samples/sec</div>
                <div class="feature-desc">Training throughput on 2x RTX 4090 GPUs</div>
              </div>
              <div class="feature-box">
                <div class="feature-icon">ðŸŽ¯</div>
                <div class="feature-title">41.6% Localization</div>
                <div class="feature-desc">Improvement in temporal action boundary detection</div>
              </div>
              <div class="feature-box">
                <div class="feature-icon">ðŸ“ˆ</div>
                <div class="feature-title">34.2% Classification</div>
                <div class="feature-desc">Downstream activity classification improvement</div>
              </div>
              <div class="feature-box">
                <div class="feature-icon">ðŸ’¾</div>
                <div class="feature-title">15.2M Parameters</div>
                <div class="feature-desc">Lightweight model (298 MB) with 2,847 FPS inference</div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Problem Statement Section -->

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Challenges in Video Understanding</h2>



            <hr class="section-divider">

            <div class="content">
              <div class="challenge-item">
                <h3 class="title is-4" style="margin-top: 0;">Challenge 1: Long-Range Temporal Dependencies</h3>
                <p>Videos inherently encode temporal information across extended sequences. A typical video contains
                  100-300 frames, requiring models to maintain effective receptive fields across time. Standard
                  transformers theoretically support this through self-attention mechanisms, but the
                  <strong>quadratic complexity O(TÂ²) in sequence length</strong> becomes prohibitively expensive,
                  scaling from 10 MB of computation for 100 frames to over 1 GB for 1000-frame videos.
                </p>
              </div>

              <div class="challenge-item">
                <h3 class="title is-4">Challenge 2: Feature Dimensionality and Computational Efficiency</h3>
                <p>Pre-extracted video features commonly have high dimensionality (e.g., 2048-D for C3D features),
                  and processing through transformers incurs <strong>O(TÂ² D) complexity</strong>. For a typical
                  250-frame video with 2048-D features, this translates to 128 million FLOPs per forward pass.
                  Multi-layer processing multiplies this burden, requiring 16-32 GB GPU memory for sequences
                  exceeding 500 frames.</p>
              </div>

              <div class="challenge-item">
                <h3 class="title is-4">Challenge 3: Representation Collapse and Triviality</h3>
                <p>Self-supervised learning in video domains is prone to representation collapse, where all frames
                  converge toward identical representations. The inherent temporal coherence of video frames makes
                  this particularly severeâ€”consecutive frames are naturally similar, causing standard contrastive
                  approaches to produce near-identical embeddings. Without explicit regularization, models
                  collapse within 10-20 epochs.</p>
              </div>

              <div class="challenge-item">
                <h3 class="title is-4">Challenge 4: Limited Annotation Data and Scalability</h3>
                <p>Frame-level or segment-level annotations are prohibitively expensive to obtain at scale. This
                  necessitates self-supervised or semi-supervised approaches that leverage unlabeled temporal
                  structure as a learning signal. Annotation quality also varies significantly across datasets,
                  with inter-annotator agreement often below 70% for temporal boundaries.</p>
              </div>
            </div>
          </div>
    </section>

    <!-- Method Overview Section -->

    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Method Overview</h2>
            <hr class="section-divider">

            <p>Our approach consists of three complementary phases designed to address the identified challenges
              while maintaining computational efficiency.</p>

            <div class="method-phase">
              <h3 class="title is-4" style="margin-top: 0; color: #000000;">Phase 1: Temporal Contrastive Learning
              </h3>
              <p><span class="badge">Core Innovation</span><span class="badge">Self-Supervised</span></p>
              <p>We enforce similarity between consecutive frames through contrastive learning of temporal
                proximity. The temporal contrastive loss is formulated as:</p>
              <div class="equation-block">
                $$\mathcal{L}_{\text{temporal}} = \sum_{i=1}^{T-1} (1 - \cos(\mathbf{h}_i, \mathbf{h}_{i+1}))$$
              </div>
              <p>This encourages temporally adjacent frames to occupy nearby positions in representation space. To
                prevent representation collapse, we employ batch-level variance regularization:</p>
              <div class="equation-block">
                $$\mathcal{L}_{\text{reg}} = \max(0, \tau - \text{std}_{\text{batch}}(\mathbf{H}))$$
              </div>
              <p>where $\tau = 0.1$ is a threshold. This regularizer maintains representation diversity while
                preventing pathological collapse.</p>
            </div>

            <div class="method-phase">
              <h3 class="title is-4" style="color: #000000;">Phase 2: Hierarchical Temporal Aggregation</h3>
              <p>We introduce multi-scale temporal pooling that creates feature hierarchies through progressive
                temporal downsampling. Rather than naive max or average pooling, we employ
                <strong>attention-weighted pooling</strong> that preserves semantically important frames during
                downsampling:
              </p>
              <div class="equation-block">
                $$\mathbf{h}^{(l)}_j = \frac{\sum_{i \in \text{window}_j} \alpha_i^{(l)} \mathbf{h}^{(l-1)}_i}{\sum_{i
                \in
                \text{window}_j} \alpha_i^{(l)}}$$
              </div>
              <p>where $\alpha_i^{(l)} = \text{softmax}(\mathbf{w}^{(l)T} \mathbf{h}^{(l-1)}_i)$ are learned
                attention weights. For a typical 250-frame video, this generates a hierarchy:
                250â†’125â†’62â†’31â†’15â†’7â†’3â†’1 frames, reducing memory requirements from O(250Â²)=62,500 to O(484)â€”a
                <strong>130x reduction</strong>.
              </p>
            </div>

            <div class="method-phase">
              <h3 class="title is-4" style="color: #000000;">Phase 3: Vision-Language Alignment (Optional)</h3>
              <p>When aligned text is available, we align visual and textual embeddings through contrastive loss:
              </p>
              <div class="equation-block">
                $$\mathcal{L}_{\text{vlm}} = -\log \frac{\exp(\text{sim}(\mathbf{v}, \mathbf{t}) / \tau_t)}{\sum_{(v',
                t')
                \in \text{batch}} \exp(\text{sim}(\mathbf{v}, \mathbf{t}') / \tau_t)}$$
              </div>
              <p>This encourages videos and their corresponding captions to have similar embeddings.</p>
            </div>
          </div>
        </div>
    </section>

    <!-- Experimental Results Section -->


    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Experimental Results</h2>
            <hr class="section-divider">


            <div class="content">
              <h3 class="title is-4">Temporal Consistency</h3>
              <table class="table is-bordered is-striped is-fullwidth">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>Score</th>
                    <th>Improvement</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Direct Transformer</td>
                    <td>0.582</td>
                    <td>â€”</td>
                  </tr>
                  <tr>
                    <td>TSN</td>
                    <td>0.598</td>
                    <td>+2.7%</td>
                  </tr>
                  <tr>
                    <td>MoCo v2</td>
                    <td>0.642</td>
                    <td>+10.3%</td>
                  </tr>
                  <tr>
                    <td>VideoClip</td>
                    <td>0.691</td>
                    <td>+18.7%</td>
                  </tr>
                  <tr style="background-color: #f0f0f0; font-weight: bold;">
                    <td>Ours (Full)</td>
                    <td>0.747</td>
                    <td style="color: #667eea;">+28.4%</td>
                  </tr>
                </tbody>
              </table>

              <h3 class="title is-4">Downstream Task Performance</h3>
              <table class="table is-bordered is-striped is-fullwidth">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>Activity Classification</th>
                    <th>Temporal Localization (IoU@0.5)</th>
                    <th>Average</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Direct Transformer</td>
                    <td>0.672</td>
                    <td>0.514</td>
                    <td>0.593</td>
                  </tr>
                  <tr>
                    <td>TSN</td>
                    <td>0.681</td>
                    <td>0.527</td>
                    <td>0.604</td>
                  </tr>
                  <tr>
                    <td>MoCo v2</td>
                    <td>0.715</td>
                    <td>0.589</td>
                    <td>0.652</td>
                  </tr>
                  <tr>
                    <td>VideoClip</td>
                    <td>0.758</td>
                    <td>0.627</td>
                    <td>0.693</td>
                  </tr>
                  <tr style="background-color: #f0f0f0; font-weight: bold;">
                    <td>Ours (Full)</td>
                    <td>0.841</td>
                    <td>0.728</td>
                    <td style="color: #667eea;">0.785</td>
                  </tr>
                </tbody>
              </table>

              <h3 class="title is-4" style="margin-top: 2rem;">Ablation Study: Component Contributions</h3>
              <table class="table is-bordered is-striped is-fullwidth">
                <thead>
                  <tr>
                    <th>Component</th>
                    <th>Temporal Consistency</th>
                    <th>Activity Classification</th>
                    <th>Temporal Localization</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Baseline (Transformer)</td>
                    <td>0.582</td>
                    <td>0.672</td>
                    <td>0.514</td>
                  </tr>
                  <tr>
                    <td>+ Temporal Contrastive Loss</td>
                    <td>0.724</span></td>
                    <td>0.801</span></td>
                    <td>0.688</span></td>
                  </tr>
                  <tr>
                    <td>+ Collapse Prevention</td>
                    <td>0.731</span></td>
                    <td>0.812</span></td>
                    <td>0.701</span></td>
                  </tr>
                  <tr>
                    <td>+ Hierarchical Aggregation</td>
                    <td>0.745</span></td>
                    <td>0.823</span></td>
                    <td>0.712</span></td>
                  </tr>
                  <tr style="background-color: #f0f0f0; font-weight: bold;">
                    <td>+ Vision-Language Alignment</td>
                    <td>0.747</span></td>
                    <td>0.841</span></td>
                    <td>0.728</span></td>
                  </tr>
                </tbody>
              </table>

              <h3 class="title is-4" style="margin-top: 2rem;">Model Efficiency</h3>
              <table class="table is-bordered is-striped is-fullwidth">
                <thead>
                  <tr>
                    <th>Metric</th>
                    <th>Value</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Model Parameters</td>
                    <td>15.2M</td>
                  </tr>
                  <tr>
                    <td>Model Size (FP32)</td>
                    <td>298 MB</td>
                  </tr>
                  <tr>
                    <td>Model Size (FP16)</td>
                    <td>149 MB</td>
                  </tr>
                  <tr>
                    <td>Inference Speed (CPU)</td>
                    <td>45 FPS</td>
                  </tr>
                  <tr>
                    <td>Inference Speed (V100 GPU)</td>
                    <td>2,847 FPS</td>
                  </tr>
                  <tr style="background-color: #f0f0f0; font-weight: bold;">
                    <td>Training Throughput (2x RTX 4090)</td>
                    <td>1,575 samples/sec</span></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
    </section>

    <!-- Dataset Section -->
    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Dataset and Evaluation Metrics</h2>
            <hr class="section-divider">
            <div class="content">
              <h3 class="title is-4">ActivityNet-1.3</h3>
              <p><strong>ActivityNet-1.3</strong> is a large-scale video understanding benchmark comprising
                <strong>13,459 videos</strong> (9,032 training, 4,427 validation) with manual temporal annotations.
                Key statistics:
              </p>
              <ul>
                <li><strong>200 activity classes</strong> spanning sports, daily activities, and complex human
                  behaviors</li>
                <li><strong>Average video duration:</strong> 117 seconds</li>
                <li><strong>Frame extraction:</strong> 1 FPS, producing 200-336 frames per video (average 250
                  frames)</li>
                <li><strong>Feature representation:</strong> 2048-D C3D features pre-trained on Sports-1M</li>
                <li><strong>Total annotations:</strong> 13,929 (average 1.04 per video)</li>
              </ul>

              <h3 class="title is-4">Evaluation Metrics</h3>
              <ul>
                <li><strong>Temporal Consistency:</strong> Average cosine similarity between consecutive frame
                  embeddings</li>
                <li><strong>Activity Classification Accuracy:</strong> Fine-tuned downstream task performance on
                  held-out test videos</li>
                <li><strong>Temporal Localization IoU@0.5:</strong> Intersection-over-Union at 50% threshold for
                  action boundary prediction</li>
                <li><strong>Feature Quality:</strong> Intra-class compactness and inter-class separation
                  measurements</li>
              </ul>
            </div>
          </div>
    </section>

    <!-- Key Contributions Section -->
    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Contributions</h2>

            <hr class="section-divider">
            <div class="content">
              <div class="result-card">
                <h3 class="title is-5">1. Temporal Contrastive Learning with Collapse Prevention</h3>
                <p>We introduce a temporal contrastive framework specifically designed for video sequences,
                  exploiting frame proximity as an inherent training signal while incorporating explicit
                  collapse-prevention regularization through batch-level feature variance constraints. Unlike
                  standard contrastive methods requiring explicit negative sampling, our approach achieves
                  consistency through temporal smoothness objectives coupled with regularization preventing
                  representation collapse while maintaining meaningful gradient flow.</p>
              </div>

              <div class="result-card">
                <h3 class="title is-5">2. Hierarchical Temporal Aggregation</h3>
                <p>We propose a hierarchical temporal aggregation mechanism using multi-scale attention-weighted
                  pooling that progressively reduces temporal resolution while preserving semantic content. This
                  architecture enables processing of long sequencesâ€”experimentally extending from 250 frames
                  (standard) to 1000+ framesâ€”without quadratic memory requirements, achieving 130x memory
                  reduction.</p>
              </div>

              <div class="result-card">
                <h3 class="title is-5">3. Comprehensive Experimental Validation</h3>
                <p>We provide extensive validation on ActivityNet-1.3 with detailed comparisons against four strong
                  baselines and ablation studies quantifying component contributions. Results demonstrate
                  substantial improvements: 28.4% gain in temporal consistency, 34.2% in activity classification,
                  41.6% in temporal localization, with each component's contribution clearly quantified.</p>
              </div>

              <div class="result-card">
                <h3 class="title is-5">4. Scalable Multi-GPU Training Framework</h3>
                <p>We implement a scalable training framework using PyTorch Distributed Data Parallel achieving
                  1,575 samples/second throughput on 2x RTX 4090 GPUs with 90% hardware utilization, demonstrating
                  near-linear scaling properties up to 4 GPUs. The resulting 15.2M-parameter model (298 MB)
                  achieves 2,847 FPS inference on V100 GPUs, making deployment practical for real-time
                  applications.</p>
              </div>
            </div>
          </div>
    </section>



    <!-- Citation Section
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-2 publication-title">Citation</h2>
      <hr class="section-divider">
      <div class="content">
        <pre><code>@article{chinda2024hierarchicalvlm,
  title={HierarchicalVLM: Efficient Long-Context Video Understanding via Hierarchical Temporal Aggregation},
  author={Chinda, Adele and Azumah, Richmond and Venkateswara, Hemanth Demakethepalli},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
      </div>
    </div>
  </section> -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content has-text-centered">
              <p style="font-size: 0.9rem; margin-top: 1rem;">
                Â© 2025 Adele Chinda
              </p>
              <p style="font-size: 0.9rem; margin-top: 1rem;">
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

</body>

</html>
